# -*- coding: utf-8 -*-
"""input256_p100_train_effnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sd5nnfGt767ItqT12foi_5xKGRwjHQrS
"""

# !unzip -q /content/drive/MyDrive/kaggle/chest_xray_detection/chest_xray_256.zip -d /content/vinbigdata

# !du -h /content/vinbigdata

# !pip install timm && pip install effdet

#placeholder for random commands
# !rm -rf wandb

import pandas as pd
import numpy as np
import torch
import torchvision
import torch.utils.data as data
import torch.nn
import torch.utils.data
from PIL import Image
import os
from datetime import datetime
import time
from pycocotools.coco import COCO
import yaml
from easydict import EasyDict

from effdet import create_model, unwrap_bench, create_loader,create_model_from_config,  DetBenchTrain, EfficientDet, create_evaluator
from effdet.config import get_efficientdet_config

from effdet.efficientdet import HeadNet
from effdet.data import resolve_input_config

from timm.optim import create_optimizer
from timm.scheduler import create_scheduler

import logging
import torch
from contextlib import suppress
from timm.utils import *
from collections import OrderedDict
import transforms as T

# Config file 

with open("./scripts/detection_config.yaml","r") as stream:
    try:
        data_yaml = yaml.load(stream, Loader=yaml.FullLoader)
        config = EasyDict(data_yaml["common"])
    except yaml.YAMLError as exc:
      print(exc)

"""# Parser Class

"""

def img_to_np(pil_img):
  np_img = np.array(pil_img, dtype=np.uint8)
  if np_img.ndim < 3:
    np_img = np.expand_dims(np_img, axis=-1)
  np_img = np.moveaxis(np_img, 2, 0)  # HWC to CHW
  return np_img

class Parser:
  def __init__(self, csv_file, split = "train"):
    self.csv = pd.read_csv(csv_file)
    self.imgs_valid = []
    self.imgs_empty = [] # empty bboxes
    self.imgs_one_dr = [] # by one radiologist only 
    self.imgs_unique = []
    if split == "train":
      self.load_anno()

  def load_anno(self):
    #csv = pd.read_csv(self.csv_file)
    csv_length = len(self.csv.index)
    df_class = self.csv['class_name']
    df_id = self.csv['image_id']
    for idx in range(1,csv_length):
      if df_id[idx-1] != df_id[idx]:
        self.imgs_unique.append(idx)
    #insert idx 0 at the beginning
    self.imgs_unique.insert(0,0) #
 

    for img_loc in self.imgs_unique:
      img_frame = self.csv.iloc[img_loc]
      if img_frame['class_name'] == "No finding":
        self.imgs_empty.append(img_loc)
      else:
        self.imgs_valid.append(img_loc)

    #return self.imgs_valid, self.imgs_empty

  def get_filename(self, index):
    """Loads filename given index from csv"""
    img_id = self.csv['image_id'].iloc[index]
    return img_id

  def get_val_img_dims(self, filename):
    filename = filename[:-4] 
    df_id = self.csv['image_id']
    id_idx = df_id[df_id.str.contains(filename)].index.values[0]
    frame = self.csv.iloc[id_idx]
    return frame['width'], frame['height']



  
  def get_anno(self, index, img_id):
    target_size = 256 #input images im using now.
    target = {}
    df_id = self.csv['image_id']
    img_instances = df_id[df_id.str.contains(img_id)].index.to_list()
    bboxes = []
    labels = []
    anomalies = []
    for img_instance in img_instances:
      frame = self.csv.iloc[img_instance]
      img_height = frame['height']
      img_width = frame['width']
      x_scale = target_size / img_width
      y_scale = target_size / img_height
      anomalies.append(frame['class_name']) # to avoid repeating anomalies.
      # if frame['class_name'] in anomalies[:-1] and len(anomalies) > 1:
      #   continue
      bbox = [frame['y_min'] * y_scale,frame['x_min'] * x_scale, frame['y_max'] * y_scale, frame['x_max'] * x_scale]
      if pd.isna(bbox[0]): # any value to ensure bbox is not empty
        
        bbox = [0.0,0.0,1.0,1.0]  

      #bbox = [frame['x_min'],frame['y_min'], frame['x_max'], frame['y_max']]
      label = frame['class_id'] 
      labels.append(label)
      bboxes.append(bbox)

    # do i want to repeat the same finding by multiple radiologists?
    bboxes = np.array(bboxes, ndmin=2, dtype=np.float32)
    labels = np.array(labels, dtype=np.int64) #shape [N] rather than [N,1]? think so. 
    area = np.array((bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])) 
    iscrowd = np.zeros((len(img_instances)), dtype=np.int64) # 0 crowds

    #img_scale = np.array([1])
    img_scale = np.array([y_scale, x_scale])
    img_scale = torch.tensor([img_scale], dtype = torch.float)
    #img_scale = img_scale.
    img_size = np.array([img_height, img_width])
    img_size = torch.tensor([img_size], dtype=torch.float)




    target = dict(img_idx = index)
    #target = dict(img_idx = image_idx) #image_size = (ih,hw) ? per dataset.py
    target['bbox'] = bboxes
    target['cls'] = labels #cls previously

    # only used for validation 
    # target['img_scale'] = img_scale
    # target['img_size'] = img_size

    return target



#Box target should be [N,4] in [yo,x0, y1,x1] format per anchors.py
#Clas target should be [N,1] 
  ## Add area,iscrowd for COCO api(not needed for training.)
  # Add img_scale and img_size here later. 
  #target['img_scale']
  #target['img_size']
  #target['area'] = area
  #target['iscrowd'] = iscrowd
  # I am removing area and iscrowd from here, and adding it
  # in coco_api, bc if I do here it gives me errors with effdet loader(tv).
  # but should work fine? let see. 
#1,0,0,1 for kaggle no finding box

"""# Dataset 

"""

class XRayDataset(data.Dataset):
  def __init__(self, root, transforms = None, split = "train"):
    self.root = root
    self.transforms = transforms
    self.split = split
    self.csv_name = [os.path.join(root, "vinbigdata/test.csv") if self.split == 'test' else "instances_train.csv"]
    self.parser = Parser(self.csv_name[0], self.split)
    self.test_imgs = list(sorted(os.listdir(os.path.join(root, "vinbigdata", self.split))))
    self.test_imgs = self.test_imgs[:6]
    
  def __getitem__(self, idx):
    if self.split != "test":
      filename = self.parser.get_filename(self.parser.imgs_unique[idx]) #imgs_valid before
      img_path = os.path.join(self.root,"vinbigdata",self.split, filename + ".png")
    # fix line below. It should only run for validating, but for training it also runs which overruns 
    # previous img_path
    img_path = os.path.join(self.root, "vinbigdata", self.split, self.test_imgs[idx]) #fix this line.
    
    
    img = Image.open(img_path).convert("RGB")
    img_id = os.path.basename(img_path)[:-4] #remove .png
    np_img = img_to_np(img)
    target = None
    if self.split == "test":
      target = {}
      filename = self.test_imgs[idx]
      img_width, img_height = self.parser.get_val_img_dims(filename)
      img_info = [img_width, img_height]
      target['filename'] = filename
      target['img_info'] = img_info
    if not target:
      target = self.parser.get_anno(idx, img_id) # of the image id instead. 
    if self.transforms is not None:
      img, target = self.transforms(img, target)
    return img, target #np_img should be default for train_effnetdataloader
    #changing to img just for validation section. 


  def __len__(self):
    if self.split == "test":
      return len(self.test_imgs)
    return len(self.parser.imgs_unique) #4394 for imgs valid and 15k for imgs_unique

# FROM https://github.com/pytorch/vision/blob/master/references/detection/coco_utils.py
# Modified super small things to fit my need 


def convert_to_coco_api(ds):
    coco_ds = COCO()
    # annotation IDs need to start at 1, not 0, see torchvision issue #1530
    ann_id = 1
    dataset = {'images': [], 'categories': [], 'annotations': []}
    categories = set()
    for img_idx in range(len(ds)):
        # find better way to get target
        # targets = ds.get_annotations(img_idx)
        img, targets = ds[img_idx]
        #image_id = targets["image_id"].item()
        image_id = targets['img_idx']
        img_dict = {}
        img_dict['id'] = image_id
        img_dict['height'] = img.shape[-2]
        img_dict['width'] = img.shape[-1]
        dataset['images'].append(img_dict)
        bboxes = targets["bbox"]
        areas = np.array((bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])) 
        areas = areas.tolist()
        # to xyxy
        bboxes[:,0:4] = bboxes[:,[1,0,3,2]]
        #  to xywh
        bboxes[:, 2:] -= bboxes[:, :2]
        bboxes = bboxes.tolist()
        labels = targets['cls'].tolist()
        #areas = targets['area'].tolist()
        iscrowd = np.zeros((len(labels)), dtype=np.int64).tolist() # 0 crowds
        #iscrowd = targets['iscrowd'].tolist()
        if 'masks' in targets:
            masks = targets['masks']
            # make masks Fortran contiguous for coco_mask
            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)
        if 'keypoints' in targets:
            keypoints = targets['keypoints']
            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()
        num_objs = len(bboxes)
        for i in range(num_objs):
            ann = {}
            ann['image_id'] = image_id
            ann['bbox'] = bboxes[i]
            ann['category_id'] = labels[i]
            categories.add(labels[i])
            ann['area'] = areas[i]
            ann['iscrowd'] = iscrowd[i]
            ann['id'] = ann_id
            if 'masks' in targets:
                ann["segmentation"] = coco_mask.encode(masks[i].numpy())
            if 'keypoints' in targets:
                ann['keypoints'] = keypoints[i]
                ann['num_keypoints'] = sum(k != 0 for k in keypoints[i][2::3])
            dataset['annotations'].append(ann)
            ann_id += 1
    dataset['categories'] = [{'id': i} for i in sorted(categories)]
    coco_ds.dataset = dataset
    coco_ds.createIndex()
    print(f'Loaded {len(ds)} annotations')
    return coco_ds

"""# Evaluator"""

# From effdet/evaluator.py. Use class evaluator
import json
from pycocotools.cocoeval import COCOeval
class Evaluator:
  def __init__(self, dataset, distributed=False, pred_yxyx = False):
    self.distributed = distributed
    self.dataset = dataset
    self.pred_yxyx = pred_yxyx
    self.img_indices = []
    self.predictions = []
    self.coco_api = convert_to_coco_api(dataset)

  def add_predictions(self, detections, target):
    if self.distributed:
        if self.distributed_device is None:
            # cache for use later to broadcast end metric
            self.distributed_device = detections.device
        synchronize()
        detections = all_gather_container(detections)
        img_indices = all_gather_container(target['img_idx'])
        if not is_main_process():
            return
    else:
        img_indices = target['img_idx']

    detections = detections.cpu().numpy()
    img_indices = img_indices.cpu().numpy()
    for img_idx, img_dets in zip(img_indices, detections):
        self.img_indices.append(img_idx)
        self.predictions.append(img_dets)


  def _coco_predictions(self):
        # generate coco-style predictions
        coco_predictions = []
        coco_ids = []
        for img_idx, img_dets in zip(self.img_indices, self.predictions):
            #img_id = self._dataset.img_ids[img_idx] #corresponds to valid samples in dataset
            img_id = img_idx #double check this is correct. should be list or dict_key[]?

            coco_ids.append(img_id)
            if self.pred_yxyx:
                # to xyxy
                img_dets[:, 0:4] = img_dets[:, [1, 0, 3, 2]]
            # to xywh
            img_dets[:, 2] -= img_dets[:, 0]
            img_dets[:, 3] -= img_dets[:, 1]
            for det in img_dets:
                score = float(det[4])
                if score < .001:  # stop when below this threshold, scores in descending order
                    break
                coco_det = dict(
                    image_id=int(img_id),
                    bbox=det[0:4].tolist(),
                    score=score,
                    category_id=int(det[5]))
                coco_predictions.append(coco_det)
        return coco_predictions, coco_ids


  def reset(self):
      self.img_indices = []
      self.predictions = []

  def evaluate(self):
    if not self.distributed or dist.get_rank() == 0:
        assert len(self.predictions)
        coco_predictions, coco_ids = self._coco_predictions()
        json.dump(coco_predictions, open('./temp.json', 'w'), indent=4)
        results = self.coco_api.loadRes('./temp.json')
        coco_eval = COCOeval(self.coco_api, results, 'bbox')
        coco_eval.params.imgIds = coco_ids  # score only ids we've used
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        metric = coco_eval.stats[0]  # mAP 0.5-0.95
        if self.distributed:
            dist.broadcast(torch.tensor(metric, device=self.distributed_device), 0)
    else:
        metric = torch.tensor(0, device=self.distributed_device)
        dist.broadcast(metric, 0)
        metric = metric.item()
    self.reset()
    return metric

"""# COCO API"""



# root = os.getcwd()
# mydata = XRayDataset(root, get_transform(train = True))
# mydata.__getitem__(2)


# # my targets should probably be xyxy, not yxyx as i have it currently.(fix?)
# coco_api = convert_to_coco_api(mydata)
# coco_api.imgs.keys()) # dict_keys[0,1,2] where 0,1,2 are the ids(index)
# coco_api.imgs # {0: {'id':0,'height':xx,'width':xx}, 1: {}}}
#  # list with [0,1,2] from the keys
# #coco_api.params.imgIds = 5 # sets the specific ids to evaluate.

"""
# Model Creation
"""



'''ATtempt 1 for model
I think this is correct, since with the pretrained argument, it already loads
the checkpoint path given the model name in the base config. Otherwise, if false,
it loads default. 
So it loads the original head, and then modifies the num_classes, so there 
is no mismatch with the coco head. 

NUM_CLASSES = 14 IF IM LEAVING NO FINDING OUT
OTHERWISE NUM_CLASSES = 15 AS ORIGINALLY DONE
'''

def get_model(model_name):
  #model_name = 'tf_efficientdet_d5' #too many parameters
  base_config = get_efficientdet_config(model_name)
  base_config.image_size = (256,256) #1024,1024
  #base_config.norm_kwargs=dict(eps=.001, momentum=.01)

  # base_config.num_classes = 909*90
  # net = create_model_from_config(base_config, bench_task='train', pretrained = True, num_classes = 15, **labeler_dict)
  model = create_model_from_config(base_config, bench_task='train', pretrained = True, num_classes = 15)
  return model

"""# Train Transforms

"""


def get_transform(train):
    transforms = []
    transforms.append(T.ToTensor())
    #commenting the horizontal flip as getting issues with size of bboxes.(too many indices dim 1)
    # bboxes width - bbox[000] blah blah
    #if train:
        #transforms.append(T.RandomHorizontalFlip(0.5))
    return T.Compose(transforms)

from effdet.anchors import Anchors, AnchorLabeler
#FIXME: args here for input_config, add to the detection_config.yaml
args = dict(interpolation=None, mean=None,std=None,fill_color=None)


def train_epoch(
        epoch, model, loader, optimizer, args,
        lr_scheduler=None, saver=None, output_dir='', amp_autocast=suppress, loss_scaler=None, model_ema=None):

    batch_time_m = AverageMeter()
    data_time_m = AverageMeter()
    losses_m = AverageMeter()

    model.train()

    end = time.time()
    last_idx = len(loader) - 1
    num_updates = epoch * len(loader)
    for batch_idx, (input, target) in enumerate(loader):
        last_batch = batch_idx == last_idx
        data_time_m.update(time.time() - end)

        if args.channels_last:
            input = input.contiguous(memory_format=torch.channels_last)

        with amp_autocast():
            output = model(input, target)
        loss = output['loss']

        if not args.distributed:
            losses_m.update(loss.item(), input.size(0))

        optimizer.zero_grad()
        if loss_scaler is not None:
            loss_scaler(loss, optimizer, clip_grad=args.clip_grad, parameters=model.parameters())
        else:
            loss.backward()
            if args.clip_grad:
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)
            optimizer.step()

        torch.cuda.synchronize()
        if model_ema is not None:
            model_ema.update(model)
        num_updates += 1

        batch_time_m.update(time.time() - end)
        if last_batch or batch_idx % args.log_interval == 0:
            lrl = [param_group['lr'] for param_group in optimizer.param_groups]
            lr = sum(lrl) / len(lrl)

            if args.distributed:
                reduced_loss = reduce_tensor(loss.data, args.world_size)
                losses_m.update(reduced_loss.item(), input.size(0))

            if args.local_rank == 0:
                print(
                    'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '
                    'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '
                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '
                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '
                    'LR: {lr:.3e}  '
                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(
                        epoch,
                        batch_idx, len(loader),
                        100. * batch_idx / last_idx,
                        loss=losses_m,
                        batch_time=batch_time_m,
                        rate=input.size(0) * args.world_size / batch_time_m.val,
                        rate_avg=input.size(0) * args.world_size / batch_time_m.avg,
                        lr=lr,
                        data_time=data_time_m))

                if args.save_images and output_dir:
                    torchvision.utils.save_image(
                        input,
                        os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),
                        padding=0,
                        normalize=True)

        if saver is not None and args.recovery_interval and (
                last_batch or (batch_idx + 1) % args.recovery_interval == 0):
            saver.save_recovery(epoch, batch_idx=batch_idx)

        if lr_scheduler is not None:
            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)

        end = time.time()
        # end for

    if hasattr(optimizer, 'sync_lookahead'):
        optimizer.sync_lookahead()

    return OrderedDict([('loss', losses_m.avg)])

"""# Training Loop"""

def validate(model, loader, args, evaluator=None, log_suffix=''):
    batch_time_m = AverageMeter()
    losses_m = AverageMeter()

    model.eval()

    end = time.time()
    last_idx = len(loader) - 1
    with torch.no_grad():
        for batch_idx, (input, target) in enumerate(loader):
            last_batch = batch_idx == last_idx
            # Add targets needed for validation only. Hardcoding single gpu cuda:0.
            # Probably need to add it to dataset.__getitem__ later instead of here
            batch_size = input.shape[0]
            img_size = input[0].shape[-2:]
            target["img_scale"] = torch.tensor([1.0] * batch_size, dtype=torch.float).to('cuda:0')
            target["img_size"] = torch.tensor([img_size] * batch_size, dtype=torch.float).to('cuda:0')

            output = model(input, target)
            loss = output['loss']

            if evaluator is not None:
                evaluator.add_predictions(output['detections'], target)

            if args.distributed:
                reduced_loss = reduce_tensor(loss.data, args.world_size)
            else:
                reduced_loss = loss.data

            torch.cuda.synchronize()

            losses_m.update(reduced_loss.item(), input.size(0))

            batch_time_m.update(time.time() - end)
            end = time.time()
            if args.local_rank == 0 and (last_batch or batch_idx % args.log_interval == 0):
                log_name = 'Test' + log_suffix
                print(
                    '{0}: [{1:>4d}/{2}]  '
                    'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '
                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '.format(
                        log_name, batch_idx, last_idx, batch_time=batch_time_m, loss=losses_m))

    metrics = OrderedDict([('loss', losses_m.avg)])
    if evaluator is not None:
        metrics['map'] = evaluator.evaluate()

    return metrics

"""# Override def parameters"""

#Override some default parameters
config.model.batch_size = 16
#config.optimizer.opt = "adam"
config.optimizer.lr = 0.01
config.model.model = "tf_efficientdet_d4" # from 3M params to 6M.
# config.output_dir = "/content/drive/MyDrive/kaggle/chest_xray_detection/output"


#d1 6M d2 8M, d3 12M, d4 20M, d5 33M parameters

# Create dataset 
def create_dataset(root):
  #root = os.getcwd() #/content/vinbigdata/train
  train_dataset = XRayDataset(root, get_transform(train = True))
  validation_dataset =  XRayDataset(root, get_transform(train = False))
  # Make a subset of datasets
  seed = 3
  # splitting from training and test sets
  torch.manual_seed(seed) 
  indices = torch.randperm(len(train_dataset)).tolist()
  value_train = int((train_dataset.__len__())*0.05) # memory consumption?
  value_coco = 10 # just to force evaluating on fewer images due to OOM
  train_dataset = torch.utils.data.Subset(train_dataset, indices[:-value_train])  # :-value_train
  validation_dataset = torch.utils.data.Subset(validation_dataset, indices[-value_train:])  # -value_train:
  # coco_val_dataset = torch.utils.data.Subset(validation_dataset, indices[-value_coco:])  # to save memory? if use this line, comment line above. 
  return train_dataset, validation_dataset        #train_dataset, validation_dataset

def run_training():
  if config.use_amp == 'native':
    loss_scaler = NativeScaler()

  decreasing = True if config.eval_metric == 'loss' else False




  optimizer = create_optimizer(config.optimizer, model)
  lr_scheduler, num_epochs = create_scheduler(config.scheduler, optimizer)
  saver = CheckpointSaver(
              model, optimizer, args=config.model, model_ema=config.model_ema, amp_scaler=loss_scaler,
              checkpoint_dir=config.output_dir, decreasing=decreasing, unwrap_fn=unwrap_bench)



  start_epoch = 0
  amp_autocast = suppress




  evaluator = Evaluator(validation_dataset, distributed= False, pred_yxyx = False)

  best_metric = None
  for epoch in range(start_epoch, num_epochs):
    train_metrics = train_epoch(epoch, model, loader_train,optimizer, config.model, lr_scheduler = lr_scheduler,
                saver = saver, output_dir = config.output_dir,amp_autocast = amp_autocast,
                loss_scaler = loss_scaler, model_ema = None)
    
    eval_metrics = validate(model, loader_eval, config.model, evaluator)
    
    if lr_scheduler is not None:
      lr_scheduler.step(epoch + 1, eval_metrics[config.eval_metric]) # from correct code


    if saver is not None:
      update_summary(epoch, train_metrics, eval_metrics, os.path.join(config.output_dir, 'summary.csv'),
                    write_header = best_metric is None)
      # save proper checkpoint with eval metric
      best_metric, best_epoch = saver.save_checkpoint(epoch=epoch, metric=eval_metrics[config.eval_metric])

  if best_metric is not None:
    print('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))

from timm.utils import * #Average Meter
import logging
from contextlib import suppress
from collections import OrderedDict


model = get_model(config.model.model)
model.cuda()
print('Model %s created, param count: %d' % (config.model.model, sum([m.numel() for m in model.parameters()])))

base_config = get_efficientdet_config(config.model.model)
base_config.image_size = (256,256) #from vinbigdata_1024
base_config.num_classes = 15 # 15 originally. changed to 14 due to leaving no finding class out. lets check
#base_config.norm_kwargs=dict(eps=.001, momentum=.01)
labeler = AnchorLabeler(   
            Anchors.from_config(base_config), base_config.num_classes, match_threshold=0.5)

input_config = resolve_input_config(args, model_config=base_config)



# root = os.getcwd()
# train_dataset, validation_dataset = create_dataset(root) 



# num_workers = 4 # recommended by dataloader warning, check that later. cpuset_checked

# #Values from the pytorch implementation  
# #transform_Fn and collate_fn not there since latest release doesnt have it.
# loader_train = create_loader(
#       train_dataset,
#       input_size=input_config['input_size'],
#       batch_size=config.model.batch_size,
#       is_training=True,
#       use_prefetcher=True,
#       re_prob=config.augmentation.re_prob,
#       re_mode=config.augmentation.re_mode,
#       re_count=config.augmentation.re_count,
#       # color_jitter=args.color_jitter,
#       # auto_augment=args.aa,
#       interpolation=input_config['interpolation'],
#       fill_color=input_config['fill_color'],
#       mean=input_config['mean'],
#       std=input_config['std'],
#       num_workers=num_workers,
#       distributed=config.model.distributed,
#       pin_mem=False,
#       anchor_labeler=labeler
#   )

# #Transform_fn and collate_fn are not in loader since latest library does not have it

# loader_eval = create_loader(
#         validation_dataset,
#         input_size=input_config['input_size'],
#         batch_size=config.model.batch_size,
#         is_training=False,
#         use_prefetcher=True,
#         interpolation=input_config['interpolation'],
#         fill_color=input_config['fill_color'],
#         mean=input_config['mean'],
#         std=input_config['std'],
#         num_workers=num_workers,
#         distributed=config.model.distributed,
#         pin_mem=False,
#         anchor_labeler=labeler
#     )

# !pip install wandb
# import wandb
# wandb.init(project="EffNets")

# run_training()

